{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (120036, 11)\n",
      "X_test shape: (30009, 11)\n",
      "y_train shape: (120036,)\n",
      "y_test shape: (30009,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/11 16:37:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/11 16:37:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/11 16:38:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/11 16:39:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/11 16:39:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Precision    Recall  F1-Score\n",
      "0     Logistic Regression   0.958261  0.956246  0.950068\n",
      "1           Decision Tree   0.911056  0.904495  0.907479\n",
      "2           Random Forest   0.958261  0.956246  0.950068\n",
      "3       Gradient Boosting   0.958261  0.956246  0.950068\n",
      "4  Multi-Layer Perceptron   0.958261  0.956246  0.950068\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../../src/data/e-commerce_processed_data.csv')\n",
    "\n",
    "# Feature and Target Separation\n",
    "X = data.drop(columns=['class'])\n",
    "y = data['class']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Define a function to train and log models with MLflow\n",
    "def train_and_log_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    with mlflow.start_run():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Log parameters, metrics, and model\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_metrics({\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1_score\": f1_score(y_test, y_pred)\n",
    "        })\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "# Train and log models with MLflow\n",
    "train_and_log_model(LogisticRegression(max_iter=1000, random_state=42), \"Logistic Regression\", X_train, y_train, X_test, y_test)\n",
    "train_and_log_model(DecisionTreeClassifier(random_state=42), \"Decision Tree\", X_train, y_train, X_test, y_test)\n",
    "train_and_log_model(RandomForestClassifier(random_state=42), \"Random Forest\", X_train, y_train, X_test, y_test)\n",
    "train_and_log_model(GradientBoostingClassifier(random_state=42), \"Gradient Boosting\", X_train, y_train, X_test, y_test)\n",
    "train_and_log_model(MLPClassifier(random_state=42), \"Multi-Layer Perceptron (MLP)\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Initialize and train the models\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "mlp_model = MLPClassifier(random_state=42)\n",
    "\n",
    "# Train the models\n",
    "lr_model.fit(X_train, y_train)\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the models\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_model,\n",
    "    \"Decision Tree\": dt_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Gradient Boosting\": gb_model,\n",
    "    \"Multi-Layer Perceptron\": mlp_model\n",
    "}\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    evaluation_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "        \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        \"F1-Score\": report[\"weighted avg\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for evaluation results\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Display the evaluation results in tabular form\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "                 Model  Precision   Recall  F1-Score\n",
      "   Logistic Regression   0.958261 0.956246  0.950068\n",
      "         Decision Tree   0.911056 0.904495  0.907479\n",
      "         Random Forest   0.958261 0.956246  0.950068\n",
      "     Gradient Boosting   0.958261 0.956246  0.950068\n",
      "Multi-Layer Perceptron   0.958261 0.956246  0.950068\n"
     ]
    }
   ],
   "source": [
    "# Display the tables\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(evaluation_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Necessary Libraries for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, LSTM, SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()\n",
    "\n",
    "# One-hot encode the target\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\OnlineClass\\AIM\\Fraud_Detection_ML_Model_in_E-commerce_and_Credit_Card_Transactions\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.4333 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 2/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.9058 - loss: 0.3122 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 3/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9067 - loss: 0.3100 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 4/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9059 - loss: 0.3120 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 5/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9056 - loss: 0.3126 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 6/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9061 - loss: 0.3115 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 7/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9056 - loss: 0.3126 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 8/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.9067 - loss: 0.3102 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 9/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - accuracy: 0.9059 - loss: 0.3119 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 10/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.9061 - loss: 0.3114 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9079 - loss: 0.3074\n",
      "CNN Accuracy: 0.9066280126571655\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train.values.reshape(-1, X_train.shape[1], 1), y_train, epochs=10, batch_size=32, validation_data=(X_test.values.reshape(-1, X_test.shape[1], 1), y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test.values.reshape(-1, X_test.shape[1], 1), y_test)\n",
    "print(f\"CNN Accuracy: {cnn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\OnlineClass\\AIM\\Fraud_Detection_ML_Model_in_E-commerce_and_Credit_Card_Transactions\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - accuracy: 0.9072 - loss: 0.4331 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 2/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.9070 - loss: 0.3094 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 3/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9067 - loss: 0.3101 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 4/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9079 - loss: 0.3075 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 5/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.9063 - loss: 0.3110 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 6/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9054 - loss: 0.3131 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 7/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.9058 - loss: 0.3123 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 8/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9063 - loss: 0.3110 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 9/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9073 - loss: 0.3087 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 10/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.9051 - loss: 0.3138 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9079 - loss: 0.3074\n",
      "RNN Accuracy: 0.9066280126571655\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model\n",
    "rnn_model = Sequential([\n",
    "    SimpleRNN(64, input_shape=(X_train.shape[1], 1), activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rnn_model.fit(X_train.values.reshape(-1, X_train.shape[1], 1), y_train, epochs=10, batch_size=32, validation_data=(X_test.values.reshape(-1, X_test.shape[1], 1), y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test.values.reshape(-1, X_test.shape[1], 1), y_test)\n",
    "print(f\"RNN Accuracy: {rnn_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - accuracy: 0.9059 - loss: 0.4341 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 2/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - accuracy: 0.9060 - loss: 0.3117 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 3/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - accuracy: 0.9065 - loss: 0.3105 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 4/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.9052 - loss: 0.3136 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 5/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - accuracy: 0.9058 - loss: 0.3121 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 6/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - accuracy: 0.9076 - loss: 0.3081 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 7/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - accuracy: 0.9072 - loss: 0.3091 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 8/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.9073 - loss: 0.3087 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 9/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7ms/step - accuracy: 0.9052 - loss: 0.3135 - val_accuracy: 0.9066 - val_loss: 0.3103\n",
      "Epoch 10/10\n",
      "\u001b[1m3752/3752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - accuracy: 0.9053 - loss: 0.3132 - val_accuracy: 0.9066 - val_loss: 0.3104\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9079 - loss: 0.3075\n",
      "LSTM Accuracy: 0.9066280126571655\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train.shape[1], 1), activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train.values.reshape(-1, X_train.shape[1], 1), y_train, epochs=10, batch_size=32, validation_data=(X_test.values.reshape(-1, X_test.shape[1], 1), y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test.values.reshape(-1, X_test.shape[1], 1), y_test)\n",
    "print(f\"LSTM Accuracy: {lstm_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and Display Evaluation Results in Tabular Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results for All Models:\n",
      "                 Model  Precision   Recall  F1-Score  Accuracy\n",
      "   Logistic Regression   0.958261 0.956246  0.950068       NaN\n",
      "         Decision Tree   0.911056 0.904495  0.907479       NaN\n",
      "         Random Forest   0.958261 0.956246  0.950068       NaN\n",
      "     Gradient Boosting   0.958261 0.956246  0.950068       NaN\n",
      "Multi-Layer Perceptron   0.958261 0.956246  0.950068       NaN\n",
      "                   CNN        NaN      NaN       NaN  0.906628\n",
      "                   RNN        NaN      NaN       NaN  0.906628\n",
      "                  LSTM        NaN      NaN       NaN  0.906628\n"
     ]
    }
   ],
   "source": [
    "# Collect evaluation results for neural networks\n",
    "nn_evaluation_results = [\n",
    "    {\"Model\": \"CNN\", \"Accuracy\": cnn_accuracy},\n",
    "    {\"Model\": \"RNN\", \"Accuracy\": rnn_accuracy},\n",
    "    {\"Model\": \"LSTM\", \"Accuracy\": lstm_accuracy}\n",
    "]\n",
    "\n",
    "# Create a DataFrame for evaluation results\n",
    "nn_evaluation_df = pd.DataFrame(nn_evaluation_results)\n",
    "\n",
    "# Combine evaluation results from all models\n",
    "all_evaluation_df = pd.concat([evaluation_df, nn_evaluation_df], ignore_index=True)\n",
    "\n",
    "# Display the combined evaluation results in tabular form\n",
    "print(\"\\nEvaluation Results for All Models:\")\n",
    "print(all_evaluation_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
